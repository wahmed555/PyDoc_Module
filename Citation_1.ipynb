{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: 'python-'\n"
     ]
    }
   ],
   "source": [
    "pip install python- docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import docx NOT python-docx \n",
    "import docx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Document class  \n",
    "# from the docx module \n",
    "from docx import Document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of a  \n",
    "# word document we want to open \n",
    "doc = Document('IEEEexample_no_hyperlinks.docx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.document.Document at 0xf7060cf168>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of paragraph objects:->>>\n",
      "[<docx.text.paragraph.Paragraph object at 0x000000F7060C2C88>, <docx.text.paragraph.Paragraph object at 0x000000F7060C2BE0>, <docx.text.paragraph.Paragraph object at 0x000000F7060C2CF8>, <docx.text.paragraph.Paragraph object at 0x000000F7060C2B70>, <docx.text.paragraph.Paragraph object at 0x000000F7060C2D68>, <docx.text.paragraph.Paragraph object at 0x000000F7060C2CC0>, <docx.text.paragraph.Paragraph object at 0x000000F7060C2080>, <docx.text.paragraph.Paragraph object at 0x000000F7060C20B8>, <docx.text.paragraph.Paragraph object at 0x000000F7060C2278>, <docx.text.paragraph.Paragraph object at 0x000000F7060C2320>, <docx.text.paragraph.Paragraph object at 0x000000F7060C2FD0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5390>, <docx.text.paragraph.Paragraph object at 0x000000F7060D53C8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5400>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5438>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5470>, <docx.text.paragraph.Paragraph object at 0x000000F7060D54A8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D54E0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5518>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5550>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5588>, <docx.text.paragraph.Paragraph object at 0x000000F7060D55C0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D55F8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5630>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5668>, <docx.text.paragraph.Paragraph object at 0x000000F7060D56A0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D56D8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5710>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5748>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5780>, <docx.text.paragraph.Paragraph object at 0x000000F7060D57B8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D57F0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5828>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5860>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5898>, <docx.text.paragraph.Paragraph object at 0x000000F7060D58D0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5908>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5940>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5978>, <docx.text.paragraph.Paragraph object at 0x000000F7060D59B0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D59E8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5A20>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5A58>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5A90>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5AC8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5B00>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5B38>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5B70>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5BA8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5BE0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5C18>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5C50>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5C88>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5CC0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5CF8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5D30>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5D68>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5DA0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5DD8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5E10>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5E48>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5E80>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5EB8>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5EF0>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5F28>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5F60>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5F98>, <docx.text.paragraph.Paragraph object at 0x000000F7060D5FD0>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE048>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE080>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE0B8>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE0F0>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE128>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE160>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE198>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE1D0>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE208>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE240>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE278>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE2B0>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE2E8>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE320>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE358>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE390>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE3C8>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE400>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE438>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE470>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE4A8>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE4E0>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE518>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE550>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE588>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE5C0>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE5F8>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE630>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE668>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE6A0>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE6D8>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE710>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE748>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE780>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE7B8>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE7F0>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE828>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE860>, <docx.text.paragraph.Paragraph object at 0x000000F7060EE898>]\n"
     ]
    }
   ],
   "source": [
    "# print the list of paragraphs in the document \n",
    "print('List of paragraph objects:->>>') \n",
    "print(doc.paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of runs objects in 1st paragraph:->>>\n",
      "[<docx.text.run.Run object at 0x000000F7060C2BA8>, <docx.text.run.Run object at 0x000000F7060C2828>]\n"
     ]
    }
   ],
   "source": [
    "# print the list of the runs  \n",
    "# in a specified paragraph \n",
    "print('\\nList of runs objects in 1st paragraph:->>>') \n",
    "print(doc.paragraphs[0].runs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text in the 1st paragraph:->>>\n",
      "The application of machine learning algorithm in underwriting process\n"
     ]
    }
   ],
   "source": [
    "# print the text in a paragraph  \n",
    "print('\\nText in the 1st paragraph:->>>') \n",
    "print(doc.paragraphs[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The whole content of the document:->>>\n",
      "\n",
      "ieeexplore.ieee.org \n",
      "The application of machine learning algorithm in underwriting process\n",
      "First Name / Given Name Family Name / Last Name / Surname\n",
      "18-22 minutes\n",
      "\n",
      "Abstract:\n",
      "This paper firstly analyses the actual underwriting methods of Chinese life insurance companies, and points out the merits and shortcomings of these methods. Then the incomplete database of insurance company is mined by the data mining's association rule algorithm. Thirdly the support vector machine (SVM) is applied to the underwriting process to classify the applicants. Finally the directions for improving this algorithm are pointed out. The algorithm proposed in this paper has promising future in underwriting process.\n",
      "Date of Conference: 18-21 Aug. 2005 \n",
      "Date Added to IEEE Xplore: 07 November 2005 \n",
      "Print ISBN: 0-7803-9091-1 \n",
      "INSPEC Accession Number: 8761678 \n",
      "Publisher: IEEE \n",
      "Conference Location: Guangzhou, China, China \n",
      "\n",
      "At present, taking into consideration of the applicant's most decisive hazards in the health statement and other extensive information regarding occupation, family background, income, and so forth has been the underwriting process of life insurance companies in China [1], [2], [3]. Firstly, let R denote the risk variable. Let X1,X2,….,Xn be the hazard variables. If there is a hazard (e.g. the applicant smokes) and this hazard is denoted by Xi, then Xi=. The influence of this hazard to the future lifetime of the applicant determines how much  is. Set Xi=0, if this hazard does not exist. After every Xi(i=1,2,…n) is evaluated, calculate RR=∑j=1nXj by. Then one can get underwriting result after considering how much R is. If R is less than one critical value, the application of policy is acceptable. Otherwise the applicant is declined. The content of personal statement and the physical examination are primarily determined by the benefit amount and the age of applicants. The larger the amount of benifit is, the stricter the health examination will be. The merit of this strategy is that the process is very simple to execute and it is in favor of cutting down the running expenses. But the shortcomings are obvious. The formers of this operation neglect the interrelationship between the hazards (the hazards may not be independent). They take the hazards into account, but they neglect the favorable information. If two different applicants, whose benefit amount, age and gender are same, have identical values of R, then they will have identical underwriting results. But they may have different risk. This operation can find high-risk applicants, but may not be good at finding moderate-risk applicants. Thus probably we could not ensure the insured pool's risk to be low enough.\n",
      "Deeper research shows that health statement and other personal information may have nonlinear connections with claim risk. The traditional underwriting process may neglect some important information. The relationship between the health statement and claim risk may be very complex and impossible to be described by a function.\n",
      "In this paper we proposed a novel algorithm that firstly mines the insured data by association rule [4], [5], trying to find the relationship among all personal statement and physical examination items. Secondly we use Support Vector Machine (SVM) [6], [7] to make the underwriting decisions. Finally, we choose the parameter of the kernel function by experiment. This is a novel algorithm that is completely different from traditional operations. From the experiment result we can see that there are great potentials for the application and the development of it.\n",
      "SECTION 2.\n",
      "Association rules (AR) and Support Vector Machine (SVM)\n",
      "AR [4], [5] mining is an important algorithm in data mining. AR mining can find interesting associations or relationships among itemsets in the large-scale database. People become more and more interested in mining association rules in their database. Finding interesting AR in large-scale business database is helpful for making business decisions.\n",
      "Let I={i1,i2,…,in} denote the set of all personal statement items and D={t1,t2,….tm} denote the set of transactions (every transaction denotes a insured with some hazards items) [5]. Each transaction tj represents a partial set tj⊆I of set I of items. When all items of set X are presented in transaction tj,tj is said to contain X(X⊆tj). The support level of X is the ratio of the number of transactions that include X to the total number D. For example, assuming X={smoking,chronic disease}, the percentage of transactions containing these two items in all transactions is the support level of X. An association rule is the rule of the form \n",
      "X⇒Y\n",
      "View Source which is established between item sets and such that X⊂I,Y⊂I, and X∩Y=ϕ. The confidence level is evaluated by \n",
      "confidence(X⇒Y)=sup port(X∪Y)sup port(X)\n",
      "View Source and the support level is evaluated by \n",
      "sup port(X⇒Y)=sup port(X∪Y)|D|\n",
      "View Source\n",
      "When the lower limits of the confidence level and the support level are given, finding association rules that satisfy these requirements from the database becomes a problem in data mining.\n",
      "Actually, the SVM [6], [7] is an excellent classification algorithm. SVM is a novel machine learning and patterning recognition algorithm based on Statistical Learning Theory (SLT). It has outperformed the traditional techniques in various applications such as handwritten digit recognition, function approximation problems and probability density estimation. The SVM is founded on Structural Risk Minimization Principle. It can take advantage of the merits of Neural Network (NN) and overcome the traditional NN's shortcoming of over-learning. It's an appropriate algorithm for underwriting.\n",
      "SECTION 3.\n",
      "Mining the history data using AR\n",
      "One of the shortcomings in traditional underwriting operation is that one may neglect the relationship between the applicants' information items. So the first step of this algorithm is to neaten and mine the insured's personal information and claim data using AR mining. We try to find the potential association and relationship between these items.\n",
      "In insurance practice, the amount of benefit determines the number of personal statement and physical examination items. In general, most small benefit and young applicants need no physical examinations. Thus the data of these policyholders is not integrated relatively.\n",
      "Before the AR mining, we must neaten the original data. In AR mining we need discrete data [4]. Thus we have to transform the continuous data into discrete data. The transformation method is to divide the definition domain of every continuous item into several intervals, then to distribute every value of the item into corresponding interval, finally to evaluate the new discrete item variable by the sequence number of the interval to which the original value belongs.\n",
      "We adopt the classic Apriori Algorithm [4] in AR mining. Apriori Algorithm is the basis of all AR algorithms in recent researches. The theory of this algorithm is based on the so-called Apriori Attribute: all non-empty subsets of the frequent itemsets must be frequent. In Apriori Algorithm we repeat two steps: connecting and trimming. In the step of connecting, we produce high dimension itemsets by connecting the low dimension ones. In order to reduce operation complexity, in the step of trimming we delete the frequent itemsets which can not exist according to Apriori Attribute. These two steps will be repeated until there is no higher dimension itemsets.\n",
      "Because of the small benefit amount applicants need not have physical examination. So the insured database lacks some information. We can't directly use the Apriori Algorithm that is designed for the integrated database. But we can estimate the actual support level of one itemset by calculating the minimum possible support ratio (when minimum including) and maximum possible support ratio (when maximum including) [8], [9].\n",
      "We may get some association or relationship between the insured information items in insurance companies' database by means of AR mining. And we may simplify the data and make some preparation for the next classification process. The data mining makes great sense for improving the underwriting algorithm.\n",
      "SECTION 4.\n",
      "Classifying policy applicants using SVM\n",
      "Let Xi=(x1,x2,…..,xn) denote the ith policyholder's (or applicant's) personal statement vector, whose element xj denotes a health item (e.g. blood pressure). Suppose there are three classes: standard class C1 (in which applicants should be accepted), substandard class C2 (in which applicants may be accepted but with some conditions) and declined class (C3) [1]. For the simplicity of the description of the model, amalgamate C1 and C2 as an accepted class C1,2. We only study the health and life insurance because the studying of health statement only makes sense for the prediction of disease claim or life status. We take the policyholders who bought health insurance and claimed at least three times or bought life insurance and died last year as C3. And take other policyholders as C1,2. Let Yi be decision variable: \n",
      "Yi={1−1high−risk−client(C3,declined) low−risk−client(C1,2,accepted)\n",
      "View Source Thus every insured or client is denoted by ⟨Xi,Yi⟩.\n",
      "As in practice some of the clients' information is untruthful, so we may accept some high-risk clients. At the same time, not all of the declined clients will fall ill or die in a short time. So this problem is a non-separable case. The client information space is an input space made up of non-separable patterns.\n",
      "Theorem (Cover, 1965) [6] tells us: the nonlinear pattern classification problem has higher probability to be linear-separable after it is projected into a high-dimensional feature space than projected into a low-dimension one.\n",
      "We classify the applicants into C1,2 and C3 using SVM [6], [7]. The nonlinear function φ(X) is used to project the applicants (or policyholders) information vectors into a high dimensional feature space Φ. Our object is to find a hyperplane in feature space to classify the clients into two classes. Firstly, assume that the input data is linear-separable after being projected into the high dimension space. And: \n",
      "φ(X)=[φ1(X),φ2(X),…,φm(X)],\n",
      "View Source where m>n.\n",
      "Then we construct a hyperplane ωTφ(X)+b=0 for distinguishing the two classes of C1,2 and C3. Among all hyperplanes separating the two categories, let us consider the one that has maximal distance to the closest vector φ(X) from the training data. We call this hyperplane the optimal hyperplane. The training data is from policyholders' information, which satisfies: \n",
      "Yi[ω10φ(Xi)+b0]≥1(1)\n",
      "View Source\n",
      "The vectors that satisfy the equation are called Support Vector Clients (SVC). \n",
      "Yi[ωT0φ(Xi)+b0]=1(2)\n",
      "View Source\n",
      "One can show that the optimal hyperplane is defined by the consisting of the vector ω0 and the constant b0 that minimize the quadratic form \n",
      "(ω)=12ωTω(3)\n",
      "View Source subject to constraints: \n",
      "Yi[ωT0φ(Xj)+b0]≥1, i=1…N(4)\n",
      "View Source\n",
      "This optimization problem is the so-called primal problem. In practice, a separating hyperplane may not exist, for example, if a high noise level causes a large overlap of the classes. To allow for the possibility of examples violating, one introduces slack variables [7]: \n",
      "ξi≥0, i=1…N(5)\n",
      "View Source along with relaxed constraints \n",
      "Yi[ωT0φ(Xi)+b0]≥1−ξi, i=1…N(6)\n",
      "View Source\n",
      "A classifier that generalizes well is then found by controlling both the classifier capacity (via 12ωTω) and the number of training errors, minimizing the objective function \n",
      "(ω,ξ)=12ωTω+C∑i=1Nξi(7)\n",
      "View Source subject to the constraints (5) and (6), for some value of the constant C>0 determining the trade-off. This constrained optimization problem is dealt with by introducing Lagrange multipliers αi≥0 and a Lagranrian \n",
      "−∑i=1Nαi[Yi(ωTφ(Xi)+b)−1+ξi]J(ω,b,α)=12ωTω+C∑i=1Nξi(8)\n",
      "View Source\n",
      "The saddle point of the Lagranrian (8) determines the solution of this problem. For this constrained optimization problem we can construct another so-called dual problem. The solution to the dual problem is the same with that to the primal problem. And the Lagrange multipliers determine the optimal solution. The dual problem is described as follows: given training data {(Xi,Yi)}Ni=1, maximize the objective function \n",
      "(α)=∑i=1Nαi−12∑i=1N∑i=1NαiαjYiYjK(Xi,Xj)(9)\n",
      "View Source to find the Lagrange multipliers {αi}Ni=1, subject to \n",
      "0≤αi≤C i=1,2,…..,N∑i=1Nαidi=0(11)(10)\n",
      "View Source where C is determined by prior knowledge.\n",
      "K(Xi,Xi) is called inner product kernel, which can be chosen from polynomial function (XTiXj+1)p (p is determined by user), radial basis function (RBF) exp(−12σ2‖Xi−Xj‖2), two-layer perceptron function tanh (β0XTiXj+β1) and so on.\n",
      "The decision function is: \n",
      "f(X)=sign(∑i=1NYiαjK(X,Xi)+b0)(12)\n",
      "View Source\n",
      "In underwriting practice, we need firstly evaluate X by the applicant's personal information data. If f(X)=−1, then accept this applicant. Otherwise we have to decline. As to the problem of dividing the class C1,2 into C1 and C2, we can use SVM again. The process is similar to the former one. Finally, the three classes of clients should be separated as in Figure 1. \n",
      "\n",
      "Figure 1. \n",
      "The underwriting result\n",
      "View All\n",
      "SECTION 5.\n",
      "Experiment results\n",
      "The data with which we do the experiment are some policyholders' data from one life-insurance company and some simulated data (the proportion is 3:4). The structure of the data is the actual health statement structure of that life insurance company, including personal information items (such as smoking, cholesterin, hypertension, occupation, Family history, fatness, cardiopathy, hepatitis, etc.) and the risk evaluation variable Y (related with the claim records or acts as the decision variable).\n",
      "Experiment process: \n",
      "Data pretreatment, including transforming the continuous data into discrete data, repair some missing data.\n",
      "Mining clients health database by AR.\n",
      "Training SVM, adopting Radial Basis Function (RBF) kernel exp(−12σ2‖Xi−Xj‖2) as the kernel function. We finally get σ=7 by considering over the training and testing correct ratio (C. R.) simultaneously (see Table 1).\n",
      "Testing.\n",
      "The algorithm of SVM is programmed in C++ language, and is compiled in Visual C++6.0 environment. After set σ from 0.1 to 1000, train and test the machine. Some of the results are recorded in Table 1. \n",
      "Table 1. Performance of different σ\n",
      "\n",
      "The experiment results testify the validity of the algorithm. But the correct ratios of training and testing are influenced by the scarcity and the missing value of the data. It is believed the correct ratios will be improved, if we would get ample data and some effective data-patching algorithm. And then the practicability will be improved.\n",
      "In this paper we apply machine learning algorithms AR and SVM to underwriting process. The experiment result testifies the validity of the algorithm. The algorithm presented in this paper acknowledges and adequately considers the interrelationships among the health-related information items. We adopt effective data mining method of AR for mining and analyzing the data. And make full use of the history data by adopting the SVM that shows good generalization performance in the practice. The process of our algorithm avoids the misplaying and blindness of doing the operation artificially. Along with the accumulation of data and experience, getting ample experiment data and finding more effective data-patching algorithm, we believe the algorithm will be more precise. It may make sense in minimizing the total risk of the insured pool and lowering down the premium as the application of the algorithm goes deeper.\n",
      "\n",
      "References\n",
      "1. X. F. Li, The practice of life-insurance actuary, Tianjin:NanKai University press, 2000.\n",
      "2. S. H. Lu, \"Information asymmetry and the Strategy of life insurance underwriting\", Insurance Studies, no. 9, pp. 39-40, Sep. 2003.\n",
      "3. X. A. Wang, \"The underwriting of annuity insurance\", Insurance Studies, no. 3, pp. 45-46, Mar. 2004.\n",
      "4. J. W. Han, M. Kamber, Data Mining: Concepts and Techniques, San Francisco:Morgan Kaufmann Publishers, 2001.\n",
      "5. S. Oyama, T. Ishida, \"Applying Association Rules to Information Navigation\", Systems and Computers in Japan, vol. 34, no. 4, Apr. 2003.\n",
      "6. S. Haykin, Neural Networks: A Comprehensive Foundation, Canada:Prentice Hall, 1999.\n",
      "7. V. Vapnik, Statistical Learning Theory, New York:John Wiley and Sons, 1998.\n",
      "8. A. Ragel, B. Cremilleux, \"Treatment of Missing Values\", Proceeding of PAKDD'98, pp. 258-269, Apr. 1998.\n",
      "9. S. W. Zhu, W. Xiong, D. B. Zhang, Y. Xiao, X. J. Chen, \"Estimation of Association Rules from Incomplete Database\", Computer Engineering, vol. 27, no. 11, pp. 39-41, Nov. 2001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for printing the complete document \n",
    "print('\\nThe whole content of the document:->>>\\n') \n",
    "for para in doc.paragraphs: \n",
    "    print(para.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# document = Document()\n",
    "doc.save('test.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APA style\n",
    "\n",
    "\n",
    "Bartz, W. R. (2002). Teaching skepticism via the CRITIC acronym and the skeptical inquirer. Skeptical Inquirer 26, 42–44.\n",
    "\n",
    "X. F. Li (2000). The practice of life-insurance actuary, Tianjin:NanKai University press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEEE style\n",
    "\n",
    "2. Bartz, W. R., Teaching skepticism via the CRITIC acronym and the skeptical inquirer. Skeptical Inquirer 26, 42–44, 2002.\n",
    "\n",
    "\n",
    "1. X. F. Li, The practice of life-insurance actuary, Tianjin:NanKai University press, 2000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "\n",
    "def docx_find_replace_text(doc, search_text, replace_text):\n",
    "    paragraphs = list(doc.paragraphs)\n",
    "    for t in doc.tables:\n",
    "        for row in t.rows:\n",
    "            for cell in row.cells:\n",
    "                for paragraph in cell.paragraphs:\n",
    "                    paragraphs.append(paragraph)\n",
    "    for p in paragraphs:\n",
    "        if search_text in p.text:\n",
    "            inline = p.runs\n",
    "            # Replace strings and retain the same style.\n",
    "            # The text to be replaced can be split over several runs so\n",
    "            # search through, identify which runs need to have text replaced\n",
    "            # then replace the text in those identified\n",
    "            started = False\n",
    "            search_index = 0\n",
    "            # found_runs is a list of (inline index, index of match, length of match)\n",
    "            found_runs = list()\n",
    "            found_all = False\n",
    "            replace_done = False\n",
    "            for i in range(len(inline)):\n",
    "\n",
    "                # case 1: found in single run so short circuit the replace\n",
    "                if search_text in inline[i].text and not started:\n",
    "                    found_runs.append((i, inline[i].text.find(search_text), len(search_text)))\n",
    "                    text = inline[i].text.replace(search_text, str(replace_text))\n",
    "                    inline[i].text = text\n",
    "                    replace_done = True\n",
    "                    found_all = True\n",
    "                    break\n",
    "\n",
    "                if search_text[search_index] not in inline[i].text and not started:\n",
    "                    # keep looking ...\n",
    "                    continue\n",
    "\n",
    "                # case 2: search for partial text, find first run\n",
    "                if search_text[search_index] in inline[i].text and inline[i].text[-1] in search_text and not started:\n",
    "                    # check sequence\n",
    "                    start_index = inline[i].text.find(search_text[search_index])\n",
    "                    check_length = len(inline[i].text)\n",
    "                    for text_index in range(start_index, check_length):\n",
    "                        if inline[i].text[text_index] != search_text[search_index]:\n",
    "                            # no match so must be false positive\n",
    "                            break\n",
    "                    if search_index == 0:\n",
    "                        started = True\n",
    "                    chars_found = check_length - start_index\n",
    "                    search_index += chars_found\n",
    "                    found_runs.append((i, start_index, chars_found))\n",
    "                    if search_index != len(search_text):\n",
    "                        continue\n",
    "                    else:\n",
    "                        # found all chars in search_text\n",
    "                        found_all = True\n",
    "                        break\n",
    "\n",
    "                # case 2: search for partial text, find subsequent run\n",
    "                if search_text[search_index] in inline[i].text and started and not found_all:\n",
    "                    # check sequence\n",
    "                    chars_found = 0\n",
    "                    check_length = len(inline[i].text)\n",
    "                    for text_index in range(0, check_length):\n",
    "                        if inline[i].text[text_index] == search_text[search_index]:\n",
    "                            search_index += 1\n",
    "                            chars_found += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    # no match so must be end\n",
    "                    found_runs.append((i, 0, chars_found))\n",
    "                    if search_index == len(search_text):\n",
    "                        found_all = True\n",
    "                        break\n",
    "\n",
    "            if found_all and not replace_done:\n",
    "                for i, item in enumerate(found_runs):\n",
    "                    index, start, length = [t for t in item]\n",
    "                    if i == 0:\n",
    "                        text = inline[index].text.replace(inline[index].text[start:start + length], str(replace_text))\n",
    "                        inline[index].text = text\n",
    "                    else:\n",
    "                        text = inline[index].text.replace(inline[index].text[start:start + length], '')\n",
    "                        inline[index].text = text\n",
    "            # print(p.text)\n",
    "\n",
    "\n",
    "# sample usage as per example \n",
    "doc = docx.Document('find_replace_test_document.docx')\n",
    "docx_find_replace_text(doc, 'Testing1', 'Test ')\n",
    "docx_find_replace_text(doc, 'Testing2', 'Test ')\n",
    "docx_find_replace_text(doc, 'rest', 'TEST')\n",
    "doc.save('find_replace_test_result.docx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
